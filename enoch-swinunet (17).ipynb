{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11675985,"sourceType":"datasetVersion","datasetId":7328074}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gzip\n\ndef get_all_path_by_ext(root, extension, match=\"\"):\n    \"\"\"\n    Retrieve all file paths in the root directory and subdirectories with a specific extension\n    and optionally filter by filenames containing a specific string.\n    \n    Parameters:\n        root (str): The root directory to search.\n        extension (str): The file extension to search for (e.g., '.gz').\n        match (str, optional): A string that the filename must contain. Defaults to \"\" (no filter).\n\n    Returns:\n        list: A list of file paths that match the given extension and match criteria.\n    \"\"\"\n    # Ensure the extension has a dot at the start\n    if not extension.startswith(\".\"):\n        extension = \".\" + extension\n    \n    file_paths = []\n    for dirpath, _, filenames in os.walk(root):\n        for filename in filenames:\n            if filename.endswith(extension) and (match in filename):\n                file_path = os.path.join(dirpath, filename).replace(\"\\\\\",\"/\")\n                file_paths.append(file_path)\n    \n    return file_paths\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:16.799479Z","iopub.execute_input":"2025-06-29T03:59:16.800067Z","iopub.status.idle":"2025-06-29T03:59:16.809848Z","shell.execute_reply.started":"2025-06-29T03:59:16.800042Z","shell.execute_reply":"2025-06-29T03:59:16.809304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\ndef load_image(img_path):\n    \"\"\"\n    Helper function to load and convert an image to RGB and return it as a numpy array.\n\n    Parameters:\n        img_path (str): Path to the image.\n\n    Returns:\n        np.ndarray: Loaded RGB image as a numpy array.\n    \"\"\"\n    img = Image.open(img_path)\n    return np.array(img)\ndef find_roi_and_crop(image, seg, padding_ratio=0.2):\n    \"\"\"\n    Find lesions based on segmentation mask and crop image with optional padding.\n    Parameters:\n        image (np.ndarray): Original image.\n        seg (np.ndarray): Segmentation mask, where the lesion region has a non-zero value.\n        padding_ratio (float): Tỉ lệ padding được thêm vào vùng tổn thương.\n    \n    Returns:\n        cropped_image (np.ndarray): Percentage of padding added to the lesion.\n    \"\"\"\n    image = cv2.resize(image, (seg.shape[0], seg.shape[1]))\n    y_indices, x_indices = np.where(seg != 0)\n    if len(x_indices) == 0 or len(y_indices) == 0:\n        return None\n    x_min, x_max = np.min(x_indices), np.max(x_indices)\n    y_min, y_max = np.min(y_indices), np.max(y_indices)\n    w = x_max - x_min\n    h = y_max - y_min\n    pad_w = int(w * padding_ratio)\n    pad_h = int(h * padding_ratio)\n    x_min = max(0, x_min - pad_w // 2)\n    y_min = max(0, y_min - pad_h // 2)\n    x_max = min(image.shape[1], x_max + pad_w // 2)\n    y_max = min(image.shape[0], y_max + pad_h // 2)\n    cropped_image = image[y_min:y_max, x_min:x_max]\n    if cropped_image.size == 0:\n        return None\n    cropped_image = cv2.resize(cropped_image, (seg.shape[0], seg.shape[1]))\n    return cropped_image\ndef plot_image(image, title=\"Image\"):\n    \"\"\"\n    Plot the image with a title.\n\n    Parameters:\n        image (np.ndarray): Image to plot.\n        title (str): Title of the plot.\n    \n    Returns:\n        None\n    \"\"\"\n    fig, a = plt.subplots()\n    a.imshow(image)\n    plt.show()\n\ndef save_image(image, save_path):\n    \"\"\"\n    Save the image to a specified path.\n\n    Parameters:\n        image (np.ndarray): Image to save. Should be in RGB format.\n        save_path (str): Path to save the image.\n    \n    Returns:\n        None\n    \"\"\"\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    img_to_save = Image.fromarray(image)\n    img_to_save.save(save_path)\n    print(f\"Image saved at: {save_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:16.831351Z","iopub.execute_input":"2025-06-29T03:59:16.831621Z","iopub.status.idle":"2025-06-29T03:59:16.883601Z","shell.execute_reply.started":"2025-06-29T03:59:16.831601Z","shell.execute_reply":"2025-06-29T03:59:16.882897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport nibabel as nib\nimport torch \nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nclass NiftiDatasetForSegmentation(Dataset):\n    def __init__(self, nii_path_tuple_list, nii_seg_path_list, volume_slices, transform_img=None, transform_mask=None):\n        self.nii_path_tuple_list = nii_path_tuple_list\n        self.nii_seg_path_list = nii_seg_path_list\n        self.volume_slices = volume_slices\n        self.transform_img = transform_img\n        self.transform_mask = transform_mask\n\n    def __len__(self):\n        return len(self.nii_path_tuple_list)\n\n    def __getitem__(self, index):\n        nii_path_tuple_case = self.nii_path_tuple_list[index]\n        nii_data_tuple_case = [nib.load(nii_path).get_fdata() for nii_path in nii_path_tuple_case]\n        nii_seg_case = nib.load(self.nii_seg_path_list[index]).get_fdata()\n\n        X, y = [], []\n        bounding_boxes = []\n\n        # Calculate the largest bounding box across all slices in `volume_slices`\n        start = nii_seg_case.shape[2] // 2 - self.volume_slices // 2\n        for j in range(self.volume_slices):\n            slice_index = j + start\n            for nii_data in nii_data_tuple_case:\n                # Find bounding box of non-zero pixel regions in each slice\n                x_nonzero, y_nonzero = torch.nonzero(torch.from_numpy(nii_data[:, :, slice_index]), as_tuple=True)\n                if x_nonzero.size(0) > 0 and y_nonzero.size(0) > 0:\n                    x_min, x_max = x_nonzero.min().item(), x_nonzero.max().item()\n                    y_min, y_max = y_nonzero.min().item(), y_nonzero.max().item()\n                    bounding_boxes.append((x_min, x_max, y_min, y_max))\n\n        # Calculate the largest enclosing bounding box across all slices\n        if bounding_boxes:\n            x_min = min(bbox[0] for bbox in bounding_boxes)\n            x_max = max(bbox[1] for bbox in bounding_boxes)\n            y_min = min(bbox[2] for bbox in bounding_boxes)\n            y_max = max(bbox[3] for bbox in bounding_boxes)\n        else:\n            # If no non-zero regions found, take the entire image\n            x_min, x_max = 0, nii_seg_case.shape[0]\n            y_min, y_max = 0, nii_seg_case.shape[1]\n\n        # Crop each image slice and mask according to the largest bounding box\n        for j in range(self.volume_slices):\n            slice_index = j + start\n\n            # Crop and add to the list for MRI image slices\n            slice_channels_data = []\n            for nii_data in nii_data_tuple_case:\n                slice_channel_data = torch.from_numpy(nii_data[:, :, slice_index][x_min:x_max+1, y_min:y_max+1]).unsqueeze(0).float()\n                slice_channels_data.append(slice_channel_data)\n            slice_channels_data_combined = torch.cat(slice_channels_data, dim=0)\n\n            # Crop and add to the list for the mask\n            nii_seg_slice = torch.from_numpy(nii_seg_case[:, :, slice_index][x_min:x_max+1, y_min:y_max+1]).unsqueeze(0).float()\n\n            # Apply transforms if provided\n            if self.transform_img:\n                slice_channels_data_combined = self.transform_img(slice_channels_data_combined)\n            if self.transform_mask:\n                nii_seg_slice = self.transform_mask(nii_seg_slice)\n\n            # Check output data types\n            if not isinstance(slice_channels_data_combined, torch.Tensor):\n                raise TypeError(f\"Expected output transform_img to be a Tensor, but got {type(slice_channels_data_combined)}.\")\n            if not isinstance(nii_seg_slice, torch.Tensor):\n                raise TypeError(f\"Expected output transform_mask to be a Tensor, but got {type(nii_seg_slice)}.\")\n\n            X.append(slice_channels_data_combined)\n            y.append(nii_seg_slice)\n\n        # Ensure the output has consistent dimensions\n        X = torch.stack(X, dim=0).permute(1, 0, 2, 3)\n        y = torch.stack(y).permute(1, 0, 2, 3)\n\n        return X, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:16.885573Z","iopub.execute_input":"2025-06-29T03:59:16.885834Z","iopub.status.idle":"2025-06-29T03:59:20.128021Z","shell.execute_reply.started":"2025-06-29T03:59:16.885815Z","shell.execute_reply":"2025-06-29T03:59:20.127401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F  \nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport pickle\nfrom torch.utils.checkpoint import checkpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:20.129385Z","iopub.execute_input":"2025-06-29T03:59:20.129791Z","iopub.status.idle":"2025-06-29T03:59:20.599769Z","shell.execute_reply.started":"2025-06-29T03:59:20.129770Z","shell.execute_reply":"2025-06-29T03:59:20.598899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install monai\n!pip install einops\nfrom monai.networks.nets import SwinUNETR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:20.600788Z","iopub.execute_input":"2025-06-29T03:59:20.601327Z","iopub.status.idle":"2025-06-29T03:59:34.536187Z","shell.execute_reply.started":"2025-06-29T03:59:20.601292Z","shell.execute_reply":"2025-06-29T03:59:34.535492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_ROOT_PATH = \"/kaggle/working/models\"\nVOLUME_SLICES = 128\nIMAGE_SIZE = (128,128)\nDATA_ROOT_PATH =\"/kaggle/input/brats-africa-small\"\nNUM_EPOCHS = 5\nOUTPUT_DIR = \"/kaggle/working/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.537038Z","iopub.execute_input":"2025-06-29T03:59:34.537824Z","iopub.status.idle":"2025-06-29T03:59:34.542443Z","shell.execute_reply.started":"2025-06-29T03:59:34.537787Z","shell.execute_reply":"2025-06-29T03:59:34.541459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nii_flair_path_list = get_all_path_by_ext(DATA_ROOT_PATH,\".nii\",\"t2f\")\nnii_t1ce_path_list = get_all_path_by_ext(DATA_ROOT_PATH,\".nii\",\"t1c\")\nnii_seg_path_list = get_all_path_by_ext(DATA_ROOT_PATH,\".nii\",\"seg\")\nnii_path_tuple_list = list(zip(nii_flair_path_list, nii_t1ce_path_list))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.544977Z","iopub.execute_input":"2025-06-29T03:59:34.545237Z","iopub.status.idle":"2025-06-29T03:59:34.718644Z","shell.execute_reply.started":"2025-06-29T03:59:34.545219Z","shell.execute_reply":"2025-06-29T03:59:34.718033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_nii_path_tuple_list, test_nii_path_tuple_list,train_nii_seg_path_list,test_nii_seg_path_list = train_test_split(nii_path_tuple_list,nii_seg_path_list,test_size=0.2,random_state=42) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.719396Z","iopub.execute_input":"2025-06-29T03:59:34.719593Z","iopub.status.idle":"2025-06-29T03:59:34.725005Z","shell.execute_reply.started":"2025-06-29T03:59:34.719576Z","shell.execute_reply":"2025-06-29T03:59:34.724249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Brats2023DataSet(NiftiDatasetForSegmentation):\n    def __getitem__(self, index):\n        X, Y = super().__getitem__(index)\n        Y = Y.squeeze(0).int()\n        Y[Y == 4] = 3 \n        Y = torch.nn.functional.one_hot(Y.long(), num_classes=4).permute(3, 0, 1, 2)\n        return X, Y.float()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.725929Z","iopub.execute_input":"2025-06-29T03:59:34.726775Z","iopub.status.idle":"2025-06-29T03:59:34.745130Z","shell.execute_reply.started":"2025-06-29T03:59:34.726749Z","shell.execute_reply":"2025-06-29T03:59:34.744515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform_img = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor(),\n])\ntransform_mask = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.745949Z","iopub.execute_input":"2025-06-29T03:59:34.746210Z","iopub.status.idle":"2025-06-29T03:59:34.764541Z","shell.execute_reply.started":"2025-06-29T03:59:34.746189Z","shell.execute_reply":"2025-06-29T03:59:34.763757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Brats2023DataSet(train_nii_path_tuple_list,train_nii_seg_path_list,VOLUME_SLICES,transform_img=transform_img,transform_mask=transform_mask)\nval_dataset = Brats2023DataSet(test_nii_path_tuple_list,test_nii_seg_path_list,VOLUME_SLICES,transform_img=transform_img,transform_mask=transform_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.765343Z","iopub.execute_input":"2025-06-29T03:59:34.766071Z","iopub.status.idle":"2025-06-29T03:59:34.782168Z","shell.execute_reply.started":"2025-06-29T03:59:34.766047Z","shell.execute_reply":"2025-06-29T03:59:34.781582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.782949Z","iopub.execute_input":"2025-06-29T03:59:34.783151Z","iopub.status.idle":"2025-06-29T03:59:34.798279Z","shell.execute_reply.started":"2025-06-29T03:59:34.783137Z","shell.execute_reply":"2025-06-29T03:59:34.797717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_loss(train_loss, valid_loss):\n    # Create a new figure and axis for training loss\n    figure_1, train_ax = plt.subplots()\n\n    # Create a new figure and axis for validation loss\n    figure_2, valid_ax = plt.subplots()\n\n    # Plot the training loss values\n    train_ax.plot(train_loss)\n    train_ax.set_xlabel('Epoch')               # Label the X-axis as 'Epoch'\n    train_ax.set_ylabel('Training Loss')       # Label the Y-axis as 'Training Loss'\n    train_ax.legend()                          # Show the legend (label box)\n\n    # Plot the validation loss values\n    valid_ax.plot(valid_loss)\n    valid_ax.set_xlabel('Epoch')               # Label the X-axis as 'Epoch'\n    valid_ax.set_ylabel('Validation Loss')     # Label the Y-axis as 'Validation Loss'\n    valid_ax.legend()                          # Show the legend (label box)\n\n    # Save the images to the output directory\n    figure_1.savefig(f\"{OUTPUT_DIR}/train_loss.png\")\n    figure_2.savefig(f\"{OUTPUT_DIR}/valid_loss.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.798958Z","iopub.execute_input":"2025-06-29T03:59:34.799156Z","iopub.status.idle":"2025-06-29T03:59:34.823701Z","shell.execute_reply.started":"2025-06-29T03:59:34.799141Z","shell.execute_reply":"2025-06-29T03:59:34.823150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass EDiceLoss(nn.Module):\n    \"\"\"Dice loss tailored to Brats need.\n    \"\"\"\n\n    def __init__(self, do_sigmoid=True):\n        super(EDiceLoss, self).__init__()\n        self.do_sigmoid = do_sigmoid\n        self.device = \"cpu\"\n\n    def binary_dice(self, inputs, targets, label_index, metric_mode=False):\n        smooth = 1.\n        if self.do_sigmoid:\n            inputs = torch.sigmoid(inputs)\n\n        if metric_mode:\n            inputs = inputs > 0.5\n            if targets.sum() == 0:\n                if inputs.sum() == 0:\n                    return torch.tensor(1., device=\"cuda\")\n                else:\n                    return torch.tensor(0., device=\"cuda\")\n        intersection = EDiceLoss.compute_intersection(inputs, targets)\n        if metric_mode:\n            dice = (2 * intersection) / ((inputs.sum() + targets.sum()) * 1.0)\n        else:\n            dice = (2 * intersection + smooth) / (inputs.pow(2).sum() + targets.pow(2).sum() + smooth)\n        if metric_mode:\n            return dice\n        return 1 - dice\n\n    @staticmethod\n    def compute_intersection(inputs, targets):\n        intersection = torch.sum(inputs * targets)\n        return intersection\n\n    def forward(self, inputs, target):\n        dice = 0\n        for i in range(target.size(1)):\n            dice = dice + self.binary_dice(inputs[:, i, ...], target[:, i, ...], i)\n\n        final_dice = dice / target.size(1)\n        return final_dice\n\n    def metric(self, inputs, target):\n        dices = []\n        for j in range(target.size(0)):\n            dice = []\n            for i in range(target.size(1)):\n                dice.append(self.binary_dice(inputs[j, i], target[j, i], i, True))\n            dices.append(dice)\n        return dices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.824344Z","iopub.execute_input":"2025-06-29T03:59:34.824554Z","iopub.status.idle":"2025-06-29T03:59:34.851028Z","shell.execute_reply.started":"2025-06-29T03:59:34.824539Z","shell.execute_reply":"2025-06-29T03:59:34.850302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SwinUNETR(\n                  in_channels=2,\n                  out_channels=4,\n                  feature_size=48,\n                  use_checkpoint=True,\n                  )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:34.851819Z","iopub.execute_input":"2025-06-29T03:59:34.852592Z","iopub.status.idle":"2025-06-29T03:59:35.453400Z","shell.execute_reply.started":"2025-06-29T03:59:34.852573Z","shell.execute_reply":"2025-06-29T03:59:35.452799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device) \nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\ncriterion = EDiceLoss().cuda()\nmetric = criterion.metric\noptimizer = AdamW(model.parameters(), lr=1e-4)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:35.455796Z","iopub.execute_input":"2025-06-29T03:59:35.456057Z","iopub.status.idle":"2025-06-29T03:59:35.676475Z","shell.execute_reply.started":"2025-06-29T03:59:35.456029Z","shell.execute_reply":"2025-06-29T03:59:35.675232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, criterion, optimizer, device):\n    train_loss_list = []\n    model.train()\n    tqdm_bar = tqdm(dataloader, total=len(dataloader))\n    for inputs, targets in tqdm_bar:\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        train_loss_list.append(loss.detach().item())\n        tqdm_bar.set_description(desc=f\"Training Loss: {loss.detach().item():.5f}\")\n        del inputs,targets, outputs, loss\n        torch.cuda.empty_cache()\n    return train_loss_list\n\ndef evaluate_one_epoch(model, dataloader, criterion, metric, device):\n    eval_loss_list = []\n    eval_dices_list= []\n    model.eval()\n\n    with torch.no_grad():\n        tqdm_bar = tqdm(dataloader, total=len(dataloader))\n        for inputs, targets in tqdm_bar:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            dices = metric(outputs, targets)\n            mean_dice = torch.tensor(dices).mean().item()\n            eval_loss_list.append(loss.detach().item())\n            eval_dices_list.extend(dices)\n            tqdm_bar.set_description(desc=f\"Valid Loss: {loss.detach().item():.5f}/Dice score:{mean_dice:.5f}\")\n            del inputs,targets, outputs, loss,dices\n            torch.cuda.empty_cache()\n    return eval_loss_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:35.677474Z","iopub.execute_input":"2025-06-29T03:59:35.677788Z","iopub.status.idle":"2025-06-29T03:59:35.685559Z","shell.execute_reply.started":"2025-06-29T03:59:35.677758Z","shell.execute_reply":"2025-06-29T03:59:35.684962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\ncheckpoint_path = None\nif checkpoint_path:\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n    loss_dict = checkpoint['loss_dict']\nelse:\n    start_epoch = 0\n    loss_dict = {'train_loss': [], 'valid_loss': []}\n\n'''\nTrain the model over all epochs\n'''\nfor epoch in range(start_epoch, NUM_EPOCHS):\n    print(\"----------Epoch {}----------\".format(epoch + 1))\n\n    # Train the model for one epoch\n    train_loss_list = train_one_epoch(model, train_dataloader, criterion, optimizer, device)\n    loss_dict['train_loss'].extend(train_loss_list)\n\n    # Run evaluation\n    eval_loss_list = evaluate_one_epoch(model, val_dataloader, criterion, metric, device)\n    loss_dict['valid_loss'].extend(eval_loss_list)\n\n    # Save model checkpoint\n    ckpt_file_name = f\"{OUTPUT_DIR}/epoch_{epoch+1}_model.pth\"\n    torch.save({\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss_dict': loss_dict,\n    }, ckpt_file_name)\n\n    plot_loss(loss_dict['train_loss'], loss_dict['valid_loss'])\n\n\nwith open(f\"{OUTPUT_DIR}/loss_dict.pkl\", \"wb\") as file:\n    pickle.dump(loss_dict, file)\n\nprint(\"Training Finished!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:59:35.686531Z","iopub.execute_input":"2025-06-29T03:59:35.686810Z","iopub.status.idle":"2025-06-29T04:07:11.761813Z","shell.execute_reply.started":"2025-06-29T03:59:35.686787Z","shell.execute_reply":"2025-06-29T04:07:11.760976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T04:07:11.762943Z","iopub.execute_input":"2025-06-29T04:07:11.763667Z","iopub.status.idle":"2025-06-29T04:07:12.099055Z","shell.execute_reply.started":"2025-06-29T04:07:11.763635Z","shell.execute_reply":"2025-06-29T04:07:12.097973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}